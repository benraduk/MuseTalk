# üéØ **GFPGAN Integration Plan for MuseTalk Pipeline**

## üìç **Integration Point Analysis**

The optimal integration point is:
- **After**: AI inference (Step 6) - `vae.decode_latents(pred_latents)`  
- **Before**: Elliptical mask cropping in `get_image()` function
- **Location**: `scripts/inference.py` line ~228, right after VAE decoding

## üöÄ **Complete Implementation Plan**

### **Phase 1: Setup & Installation (Day 1)**

#### **Step 1.1: Install GFPGAN Dependencies**
```bash
# Add to requirements.txt
pip install gfpgan
pip install basicsr>=1.4.2
pip install facexlib>=0.2.5
pip install realesrgan>=0.3.0
```

#### **Step 1.2: Download GFPGAN Models**
Create `download_gfpgan_models.py`:
```python
import os
import requests
from pathlib import Path

def download_gfpgan_models():
    models_dir = Path("models/gfpgan")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # GFPGAN v1.4 model (best for faces)
    model_url = "https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth"
    model_path = models_dir / "GFPGANv1.4.pth"
    
    if not model_path.exists():
        print("Downloading GFPGAN model...")
        response = requests.get(model_url, stream=True)
        with open(model_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"Downloaded: {model_path}")

if __name__ == "__main__":
    download_gfpgan_models()
```

#### **Step 1.3: Create GFPGAN Wrapper Class**
Create `musetalk/utils/gfpgan_enhancer.py`:
```python
import torch
import cv2
import numpy as np
from gfpgan import GFPGANer
import os
from typing import List, Union

class GFPGANEnhancer:
    """
    GFPGAN face enhancement wrapper for MuseTalk pipeline
    """
    
    def __init__(self, 
                 model_path="models/gfpgan/GFPGANv1.4.pth",
                 upscale=2,
                 arch='clean',
                 channel_multiplier=2,
                 bg_upsampler=None,
                 device=None):
        """
        Initialize GFPGAN enhancer
        
        Args:
            model_path: Path to GFPGAN model
            upscale: Upscaling factor (1, 2, 4)
            arch: GFPGAN architecture ('original', 'clean', 'RestoreFormer')
            channel_multiplier: Channel multiplier for generator
            bg_upsampler: Background upsampler (None, 'realesrgan')
            device: Device to run on ('cuda' or 'cpu')
        """
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
            
        print(f"üé® Initializing GFPGAN on {self.device}")
        
        # Initialize GFPGAN
        self.restorer = GFPGANer(
            model_path=model_path,
            upscale=upscale,
            arch=arch,
            channel_multiplier=channel_multiplier,
            bg_upsampler=bg_upsampler,
            device=str(self.device)
        )
        
        self.upscale = upscale
        print(f"‚úÖ GFPGAN initialized with {upscale}x upscaling")
    
    def enhance_face(self, face_image: np.ndarray) -> np.ndarray:
        """
        Enhance a single face image
        
        Args:
            face_image: Input face image (H, W, 3) in BGR format
            
        Returns:
            Enhanced face image (H*upscale, W*upscale, 3) in BGR format
        """
        try:
            # GFPGAN expects BGR format (which matches MuseTalk's format)
            _, _, restored_face = self.restorer.enhance(
                face_image, 
                has_aligned=False, 
                only_center_face=True,
                paste_back=True
            )
            
            return restored_face
            
        except Exception as e:
            print(f"‚ö†Ô∏è GFPGAN enhancement failed: {e}")
            # Fallback: return original image upscaled with basic interpolation
            if self.upscale > 1:
                h, w = face_image.shape[:2]
                return cv2.resize(face_image, (w * self.upscale, h * self.upscale), 
                                interpolation=cv2.INTER_LANCZOS4)
            return face_image
    
    def enhance_batch(self, face_images: List[np.ndarray]) -> List[np.ndarray]:
        """
        Enhance a batch of face images
        
        Args:
            face_images: List of face images
            
        Returns:
            List of enhanced face images
        """
        enhanced_faces = []
        for i, face in enumerate(face_images):
            print(f"üé® Enhancing face {i+1}/{len(face_images)}")
            enhanced_face = self.enhance_face(face)
            enhanced_faces.append(enhanced_face)
        
        return enhanced_faces
    
    def resize_to_original(self, enhanced_face: np.ndarray, original_size: tuple) -> np.ndarray:
        """
        Resize enhanced face back to original size if needed
        
        Args:
            enhanced_face: Enhanced face image
            original_size: (width, height) of original face
            
        Returns:
            Resized face image
        """
        if enhanced_face.shape[:2][::-1] != original_size:
            return cv2.resize(enhanced_face, original_size, interpolation=cv2.INTER_LANCZOS4)
        return enhanced_face
```

### **Phase 2: Pipeline Integration (Day 2)**

#### **Step 2.1: Modify Configuration**
Update `configs/inference/test.yaml`:
```yaml
task_0:
 video_path: "data/video/Canva_en_short.mp4"
 audio_path: "data/audio/Canva_FR_short.m4a"
 bbox_shift: -9
 result_name: "Canva_en_short-9.mp4"
 result_dir: "./results/optimized"
 
# Add GFPGAN configuration
gfpgan:
  enabled: true
  model_path: "models/gfpgan/GFPGANv1.4.pth"
  upscale: 2
  arch: "clean"
  channel_multiplier: 2
```

#### **Step 2.2: Modify Main Inference Script**
Update `scripts/inference.py`:

```python
# Add imports at the top
from musetalk.utils.gfpgan_enhancer import GFPGANEnhancer

# Add GFPGAN initialization in main() function, around line 50:
@torch.no_grad()
def main(args):
    # ... existing code ...
    
    # Initialize GFPGAN if enabled
    gfpgan_enhancer = None
    if hasattr(cfg, 'gfpgan') and cfg.gfpgan.get('enabled', False):
        print("üé® Initializing GFPGAN face enhancer...")
        gfpgan_enhancer = GFPGANEnhancer(
            model_path=cfg.gfpgan.get('model_path', 'models/gfpgan/GFPGANv1.4.pth'),
            upscale=cfg.gfpgan.get('upscale', 2),
            arch=cfg.gfpgan.get('arch', 'clean'),
            channel_multiplier=cfg.gfpgan.get('channel_multiplier', 2),
            device=device
        )
    
    # ... rest of existing code ...

# Modify the inference loop around line 228:
# UNet inference for lip-sync generation
pred_latents = unet.model(latent_batch, timesteps, encoder_hidden_states=audio_feature_batch).sample
recon = vae.decode_latents(pred_latents)

# üé® ADD GFPGAN ENHANCEMENT HERE
if gfpgan_enhancer is not None:
    print(f"üé® Applying GFPGAN enhancement to {len(recon)} faces...")
    enhanced_recon = []
    for face_idx, face_img in enumerate(recon):
        # Convert to BGR format for GFPGAN
        face_bgr = cv2.cvtColor(face_img.astype(np.uint8), cv2.COLOR_RGB2BGR)
        
        # Enhance with GFPGAN
        enhanced_face_bgr = gfpgan_enhancer.enhance_face(face_bgr)
        
        # Resize back to original size (256x256) for compatibility
        original_size = (face_img.shape[1], face_img.shape[0])  # (width, height)
        enhanced_face_bgr = gfpgan_enhancer.resize_to_original(enhanced_face_bgr, original_size)
        
        # Convert back to RGB format
        enhanced_face_rgb = cv2.cvtColor(enhanced_face_bgr, cv2.COLOR_BGR2RGB)
        enhanced_recon.append(enhanced_face_rgb)
    
    recon = enhanced_recon
    print("‚úÖ GFPGAN enhancement completed")

# Continue with existing processing...
```

#### **Step 2.3: Add Command Line Arguments**
Update `scripts/inference.py` argument parser:
```python
parser.add_argument("--enable_gfpgan", action="store_true", 
                   help="Enable GFPGAN face enhancement")
parser.add_argument("--gfpgan_upscale", type=int, default=2,
                   help="GFPGAN upscaling factor (1, 2, 4)")
parser.add_argument("--gfpgan_model", type=str, default="models/gfpgan/GFPGANv1.4.pth",
                   help="Path to GFPGAN model")
```

### **Phase 3: Optimization & Acceleration (Day 3)**

#### **Step 3.1: CUDA Optimization**
GFPGAN benefits significantly from CUDA acceleration:
- **GPU Memory**: ~2-4GB additional for GFPGAN model
- **Speed**: 5-10x faster on GPU vs CPU
- **TensorRT**: Can be optimized with TensorRT for production

#### **Step 3.2: Batch Processing Optimization**
Update `musetalk/utils/gfpgan_enhancer.py` for better batching:
```python
def enhance_batch_optimized(self, face_images: List[np.ndarray], batch_size: int = 4) -> List[np.ndarray]:
    """
    Optimized batch processing for GFPGAN
    """
    enhanced_faces = []
    
    # Process in smaller batches to manage GPU memory
    for i in range(0, len(face_images), batch_size):
        batch = face_images[i:i + batch_size]
        
        # Process batch
        batch_results = []
        for face in batch:
            enhanced = self.enhance_face(face)
            batch_results.append(enhanced)
        
        enhanced_faces.extend(batch_results)
        
        # Clear GPU cache periodically
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    return enhanced_faces
```

#### **Step 3.3: Memory Management**
Add memory monitoring:
```python
def monitor_gpu_memory():
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"GPU Memory: {memory_used:.2f}GB / {memory_total:.2f}GB")
```

### **Phase 4: Testing & Validation (Day 4)**

#### **Step 4.1: Create Test Script**
Create `test_gfpgan_integration.py`:
```python
import cv2
import numpy as np
from musetalk.utils.gfpgan_enhancer import GFPGANEnhancer

def test_gfpgan():
    # Initialize enhancer
    enhancer = GFPGANEnhancer()
    
    # Test with sample face image
    test_image = cv2.imread("assets/demo/man/man.png")
    if test_image is None:
        print("Test image not found")
        return
    
    # Enhance
    enhanced = enhancer.enhance_face(test_image)
    
    # Save results
    cv2.imwrite("test_original.png", test_image)
    cv2.imwrite("test_enhanced.png", enhanced)
    
    print("Test completed. Check test_original.png and test_enhanced.png")

if __name__ == "__main__":
    test_gfpgan()
```

#### **Step 4.2: Update Batch Script**
Update `run_inference_optimized.bat`:
```batch
@echo off
echo üöÄ Starting MuseTalk Optimized Inference with GFPGAN...

REM Set Python path
set PYTHONPATH=%PYTHONPATH%;%CD%

REM Download GFPGAN models if needed
python download_gfpgan_models.py

REM Run inference with GFPGAN enhancement
python scripts/inference.py ^
    --inference_config configs/inference/test.yaml ^
    --unet_config ./models/musetalkV15/musetalk.json ^
    --batch_size 32 ^
    --use_saved_coord ^
    --enable_gfpgan ^
    --gfpgan_upscale 2

echo ‚úÖ Inference completed!
pause
```

## üìä **Performance Expectations**

### **Quality Improvements**
- **Resolution**: 256x256 ‚Üí 512x512 (with 2x upscaling)
- **Detail Enhancement**: Better lip texture, teeth definition
- **Artifact Reduction**: Smoother skin, reduced VAE artifacts
- **Natural Look**: More realistic facial features

### **Performance Impact**
- **Speed**: +30-50% processing time (GPU)
- **Memory**: +2-4GB GPU memory
- **Quality**: Significant visual improvement

### **CUDA & TensorRT Benefits**
- **CUDA**: 5-10x faster than CPU
- **TensorRT**: Additional 20-30% speedup possible
- **Mixed Precision**: Can use FP16 for faster inference

## üéØ **Files Modified Summary**

### **New Files Created**:
1. `musetalk/utils/gfpgan_enhancer.py` - GFPGAN wrapper
2. `download_gfpgan_models.py` - Model downloader
3. `test_gfpgan_integration.py` - Testing script

### **Files Modified**:
1. `scripts/inference.py` - Main integration point
2. `configs/inference/test.yaml` - Configuration
3. `run_inference_optimized.bat` - Batch script
4. `requirements.txt` - Dependencies

### **Integration Point**: 
Line ~228 in `scripts/inference.py`, immediately after `recon = vae.decode_latents(pred_latents)`

## üîß **Technical Details**

### **GFPGAN Model Information**
- **Model Size**: ~350MB
- **Input**: 256x256 face images
- **Output**: Enhanced faces (256x256, 512x512, or 1024x1024)
- **Architecture**: GAN-based face restoration
- **Specialty**: Facial feature enhancement, artifact removal

### **Memory Requirements**
- **CPU**: 4GB+ RAM recommended
- **GPU**: 6GB+ VRAM recommended (8GB+ for batch processing)
- **Storage**: 500MB for models

### **Compatibility**
- **PyTorch**: 1.7+
- **CUDA**: 10.2+ (for GPU acceleration)
- **Python**: 3.7+

## üö® **Important Notes**

1. **Color Space**: GFPGAN expects BGR format, MuseTalk uses RGB - conversion is handled in the wrapper
2. **Size Compatibility**: Enhanced faces are resized back to 256x256 to maintain pipeline compatibility
3. **Fallback**: If GFPGAN fails, the system falls back to basic upscaling
4. **Memory Management**: GPU cache is cleared periodically to prevent OOM errors
5. **Quality vs Speed**: Upscaling factor can be adjusted (1x=fastest, 4x=highest quality)

## üìù **Implementation Checklist**

### **Phase 1: Setup**
- [ ] Install GFPGAN dependencies
- [ ] Download GFPGAN models
- [ ] Create GFPGAN wrapper class
- [ ] Test GFPGAN standalone

### **Phase 2: Integration**
- [ ] Update configuration files
- [ ] Modify inference script
- [ ] Add command line arguments
- [ ] Test integration

### **Phase 3: Optimization**
- [ ] Implement batch processing
- [ ] Add memory management
- [ ] Optimize for CUDA
- [ ] Performance testing

### **Phase 4: Validation**
- [ ] Create test scripts
- [ ] Update batch files
- [ ] Quality comparison
- [ ] Documentation update

---

*This systematic approach ensures clean integration with fallback mechanisms and optimal performance. The enhancement happens exactly after AI inference but before the elliptical mask cropping in the blending process.*
