@echo off
REM MuseTalk OPTIMIZED Inference - Maximum Performance
REM Optimized inference batch sizes + coordinate caching

echo ğŸš€ MuseTalk OPTIMIZED Inference (High Performance Mode)
echo ğŸ“ Working directory: %CD%
echo ğŸ Setting PYTHONPATH to include current directory
echo ============================================================

REM Set PYTHONPATH to include current directory
set PYTHONPATH=%PYTHONPATH%;%CD%

REM Activate conda environment if not already active
if not "%CONDA_DEFAULT_ENV%"=="musetalk" (
    echo ğŸ”„ Activating musetalk environment...
    call conda activate musetalk
)

REM Run OPTIMIZED inference with optimized settings
echo âš¡ Running with optimized settings for maximum speed...
echo ğŸ“Š Batch size: 16 (AI inference optimization)
echo ğŸ“ Output directory: ./results/optimized/
python scripts/inference.py --enable_gpen_bfr --inference_config configs/inference/test.yaml --unet_config ./models/musetalkV15/musetalk.json --batch_size 16 --result_dir "./results"

REM Restore original directory
echo ============================================================
echo âœ… Inference completed
