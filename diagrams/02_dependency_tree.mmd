graph TD
    %% DEPENDENCY TREE - SURGICAL INTEGRATION
    
    %% INPUT LAYER
    A[Input: Video + Audio] --> B[scripts/inference.py]
    
    %% PREPROCESSING DEPENDENCIES
    B --> C[Face Detection Module]
    B --> D[Audio Processing Module]
    
    %% Face Detection Dependencies (MUSETALK - KEEP)
    C --> C1["musetalk/utils/face_detection/<br/>ðŸŸ¢ FaceAlignment, LandmarksType"]
    C1 --> C1a["Dependencies:<br/>â€¢ torch<br/>â€¢ opencv-python<br/>â€¢ numpy"]
    
    C --> C2["musetalk/utils/preprocessing/<br/>ðŸŸ¢ get_landmark_and_bbox_enhanced()"]
    C2 --> C2a["Dependencies:<br/>â€¢ torch<br/>â€¢ numpy<br/>â€¢ cv2"]
    
    %% Audio Processing Dependencies (MUSETALK - KEEP)
    D --> D1["musetalk/whisper/<br/>ðŸŸ¢ WhisperModel"]
    D1 --> D1a["Dependencies:<br/>â€¢ transformers==4.48.0<br/>â€¢ torch==2.2.0<br/>â€¢ librosa==0.11.0"]
    
    %% DATA GENERATION DEPENDENCIES (MUSETALK ENHANCED)
    B --> E[Data Generation Module]
    E --> E1["musetalk/utils/utils/<br/>ðŸŸ¢ datagen_enhanced()"]
    E1 --> E1a["Dependencies:<br/>â€¢ torch<br/>â€¢ numpy<br/>â€¢ itertools"]
    
    %% VAE ENCODING DEPENDENCIES (MUSETALK - KEEP)
    B --> F[VAE Encoding Module]
    F --> F1["musetalk/models/vae/<br/>ðŸŸ¢ VAE.encode()"]
    F1 --> F1a["Dependencies:<br/>â€¢ diffusers==0.32.2<br/>â€¢ torch==2.2.0"]
    
    %% SURGICAL INFERENCE POINT
    B --> G["ðŸ”„ SURGICAL POINT:<br/>UNet Inference"]
    
    %% MuseTalk UNet (FALLBACK)
    G --> G1["musetalk/models/unet/<br/>ðŸŸ¡ UNet.model() - FALLBACK"]
    G1 --> G1a["Dependencies:<br/>â€¢ torch==2.2.0<br/>â€¢ diffusers (old version)"]
    
    %% LatentSync UNet3D (PRIMARY)
    G --> G2["LatentSync/latentsync/models/unet/<br/>ðŸ”´ UNet3DConditionModel - PRIMARY"]
    G2 --> G2a["Dependencies:<br/>â€¢ diffusers==0.32.2<br/>â€¢ torch==2.2.0<br/>â€¢ einops"]
    
    %% Surgical Wrapper (NEW)
    G --> G3["scripts/hybrid_inference.py<br/>ðŸ†• surgical_unet3d_inference()"]
    G3 --> G3a["Dependencies:<br/>â€¢ torch==2.2.0<br/>â€¢ diffusers==0.32.2<br/>â€¢ Both UNet models"]
    
    %% VAE DECODING (MUSETALK - KEEP)
    B --> H[VAE Decoding Module]
    H --> H1["musetalk/models/vae/<br/>ðŸŸ¢ VAE.decode_latents()"]
    H1 --> H1a["Dependencies:<br/>â€¢ diffusers==0.32.2<br/>â€¢ torch==2.2.0"]
    
    %% FRAME BLENDING (MUSETALK ENHANCED)
    B --> I[Frame Blending Module]
    I --> I1["musetalk/utils/blending/<br/>ðŸŸ¢ get_image()"]
    I1 --> I1a["Dependencies:<br/>â€¢ opencv-python<br/>â€¢ numpy"]
    
    %% OUTPUT
    B --> J[Video Output]
    J --> J1["Dependencies:<br/>â€¢ opencv-python<br/>â€¢ ffmpeg-python"]
    
    %% ELIMINATED DEPENDENCIES (SHOWN FOR CONTEXT)
    K["âŒ ELIMINATED CONFLICTS"]
    K --> K1["mmpose, mmcv, mmdet<br/>(MMLab ecosystem)"]
    K --> K2["tensorflow, tensorboard<br/>(Training dependencies)"]
    K --> K3["insightface, mediapipe<br/>(LatentSync preprocessing)"]
    K --> K4["onnxruntime-gpu, face-alignment<br/>(Redundant components)"]
    
    %% STYLING
    classDef keep fill:#90EE90,stroke:#006400,stroke-width:2px
    classDef surgical fill:#FFB6C1,stroke:#DC143C,stroke-width:2px
    classDef new fill:#87CEEB,stroke:#4682B4,stroke-width:2px
    classDef eliminated fill:#FFCCCB,stroke:#8B0000,stroke-width:2px
    classDef fallback fill:#FFF8DC,stroke:#DAA520,stroke-width:2px
    
    class C1,C2,D1,E1,F1,H1,I1 keep
    class G2,G2a surgical
    class G3,G3a new
    class K,K1,K2,K3,K4 eliminated
    class G1,G1a fallback